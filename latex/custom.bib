% A %

% B %

% C %

% D %

% P $
% article -- journal article
% inProceedings -- conference proceedings
% misc -- news articles, medium.com, ...
% PhDThesis -- 
% MSCSThesis --

@article{annepaka2024large,
  title={Large language models: A survey of their development, capabilities, and applications},
  author={Annepaka, Yadagiri and Pakray, Partha},
  journal={Knowledge and Information Systems},
  pages={1--56},
  year={2024},
  publisher={Springer}
}


@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

@article{jiang2024survey,
  title={A Survey on Large Language Models for Code Generation},
  author={Jiang, Juyong and Wang, Fan and Shen, Jiasi and Kim, Sungju and Kim, Sunghun},
  journal={arXiv preprint arXiv:2406.00515},
  year={2024}
}

@techreport{chowdhury2024swebench,
    title = {Introducing {SWE}-bench {Verified}},
    author = {Chowdhury, Neil and Aung, James and Shern, Chan Jun and Jaffe, Oliver and Sherburn, Dane and Starace, Giulio and Mays, Evan and Dias, Rachel and Aljubeh, Marwan and Glaese, Mia and Jimenez, Carlos E. and Yang, John and Liu, Kevin and Madry, Aleksander},
    institution = {OpenAI},
    year = {2024},
    month = {August},
    day = {13},
    url = {https://openai.com/index/introducing-swe-bench-verified/},
    note = {OpenAI Technical Report}
}

@techreport{schluntz2024swebench,
  title={Raising the bar on {SWE}-bench {V}erified with {C}laude 3.5 {S}onnet},
  author={Schluntz, Erik and Biggs, Simon and Drain, Dawn and Christiansen, Eric},
  year={2024},
  month={Oct},
  institution={Anthropic},
  url={https://www.anthropic.com/research/swe-bench-sonnet}
}

@techreport{mallick2024gemini,
  title={The next chapter of the {G}emini era for developers},
  author={Mallick, Shrestha Basu and Korevec, Kathy},
  year={2024},
  month={Dec},
  institution={Google},
  url={https://developers.googleblog.com/en/the-next-chapter-of-the-gemini-era-for-developers/}
}

@article{dohmke2023sea,
  title={Sea change in software development: Economic and productivity analysis of the ai-powered developer lifecycle},
  author={Dohmke, Thomas and Iansiti, Marco and Richards, Greg},
  journal={arXiv preprint arXiv:2306.15033},
  year={2023}
}

@article{liu2024exploring,
  title={Exploring and evaluating hallucinations in llm-powered code generation},
  author={Liu, Fang and Liu, Yang and Shi, Lin and Huang, Houkun and Wang, Ruifeng and Yang, Zhen and Zhang, Li and Li, Zhongqi and Ma, Yuchi},
  journal={arXiv preprint arXiv:2404.00971},
  year={2024}
}

@article{zhang2021survey,
  title={A survey on neural network interpretability},
  author={Zhang, Yu and Ti{\v{n}}o, Peter and Leonardis, Ale{\v{s}} and Tang, Ke},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence},
  volume={5},
  number={5},
  pages={726--742},
  year={2021},
  publisher={IEEE}
}

@article{bereska2024mechanistic,
  title={Mechanistic Interpretability for AI Safety--A Review},
  author={Bereska, Leonard and Gavves, Efstratios},
  journal={arXiv preprint arXiv:2404.14082},
  year={2024}
}

@article{wang2022interpretability,
  title={Interpretability in the wild: a circuit for indirect object identification in gpt-2 small},
  author={Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2211.00593},
  year={2022}
}

@article{hanna2024does,
  title={How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model},
  author={Hanna, Michael and Liu, Ollie and Variengien, Alexandre},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{lieberum2023does,
  title={Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla},
  author={Lieberum, Tom and Rahtz, Matthew and Kram{\'a}r, J{\'a}nos and Nanda, Neel and Irving, Geoffrey and Shah, Rohin and Mikulik, Vladimir},
  journal={arXiv preprint arXiv:2307.09458},
  year={2023}
}

@article{chughtai2024summing,
  title={Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs},
  author={Chughtai, Bilal and Cooney, Alan and Nanda, Neel},
  journal={arXiv preprint arXiv:2402.07321},
  year={2024}
}

@article{lan2023locating,
  title={Locating Cross-Task Sequence Continuation Circuits in Transformers},
  author={Lan, Michael and Barez, Fazl},
  journal={arXiv preprint arXiv:2311.04131},
  year={2023}
}

@article{elhage2022superposition,
   title={Toy Models of Superposition},
   author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/toy\_model/index.html}
}

@article{arora2018linear,
  title={Linear algebraic structure of word senses, with applications to polysemy},
  author={Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
  journal={Transactions of the Association for Computational Linguistics},
  volume={6},
  pages={483--495},
  year={2018},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{olah2020zoom,
  title={Zoom in: An introduction to circuits},
  author={Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal={Distill},
  volume={5},
  number={3},
  pages={e00024--001},
  year={2020}
}

@article{mu2020compositional,
  title={Compositional explanations of neurons},
  author={Mu, Jesse and Andreas, Jacob},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17153--17163},
  year={2020}
}

@article{elhage2022solu,
   title={Softmax Linear Units},
   author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Nanda, Neel and Henighan, Tom and Johnston, Scott and ElShowk, Sheer and Joseph, Nicholas and DasSarma, Nova and Mann, Ben and Hernandez, Danny and Askell, Amanda and Ndousse, Kamal and Jones, Andy and Drain, Dawn and Chen, Anna and Bai, Yuntao and Ganguli, Deep and Lovitt, Liane and Hatfield-Dodds, Zac and Kernion, Jackson and Conerly, Tom and Kravec, Shauna and Fort, Stanislav and Kadavath, Saurav and Jacobson, Josh and Tran-Johnson, Eli and Kaplan, Jared and Clark, Jack and Brown, Tom and McCandlish, Sam and Amodei, Dario and Olah, Christopher},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/solu/index.html}
}

@misc{nanda2022transformerlens,
    title = {TransformerLens},
    author = {Neel Nanda and Joseph Bloom},
    year = {2022},
    howpublished = {\url{https://github.com/TransformerLensOrg/TransformerLens}},
}

@misc{bloom2024saetrainingcodebase,
   title = {SAELens},
   author = {Joseph Bloom, Curt Tigges and David Chanin},
   year = {2024},
   howpublished = {\url{https://github.com/jbloomAus/SAELens}},
}

@article{nanda2023emergent,
  title={Emergent linear representations in world models of self-supervised sequence models},
  author={Nanda, Neel and Lee, Andrew and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2309.00941},
  year={2023}
}

@article{engels2024not,
  title={Not all language model features are linear},
  author={Engels, Joshua and Michaud, Eric J and Liao, Isaac and Gurnee, Wes and Tegmark, Max},
  journal={arXiv preprint arXiv:2405.14860},
  year={2024}
}

@article{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{alain2016understanding,
  title={Understanding intermediate layers using linear classifier probes},
  author={Alain, Guillaume},
  journal={arXiv preprint arXiv:1610.01644},
  year={2016}
}

@article{belinkov2022probing,
  title={Probing classifiers: Promises, shortcomings, and advances},
  author={Belinkov, Yonatan},
  journal={Computational Linguistics},
  volume={48},
  number={1},
  pages={207--219},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{o2023disentangling,
  title={Disentangling neuron representations with concept vectors},
  author={O'Mahony, Laura and Andrearczyk, Vincent and M{\"u}ller, Henning and Graziani, Mara},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3770--3775},
  year={2023}
}

@inproceedings{tigges2024language,
  title={Language models linearly represent sentiment},
  author={Tigges, Curt and Hollinsworth, Oskar John and Geiger, Atticus and Nanda, Neel},
  booktitle={ICML 2024 Workshop on Mechanistic Interpretability},
  year={2024}
}

@article{arditi2024refusal,
  title={Refusal in language models is mediated by a single direction},
  author={Arditi, Andy and Obeso, Oscar and Syed, Aaquib and Paleka, Daniel and Panickssery, Nina and Gurnee, Wes and Nanda, Neel},
  journal={arXiv preprint arXiv:2406.11717},
  year={2024}
}

@article{bricken2023monosemanticity,
   title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
   author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
   year={2023},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
}

@misc{sharkey2022taking,
    author = {Sharkey, Lee and Braun, Dan and Millidge, Beren},
    title = {Taking Features Out of Superposition with Sparse Autoencoders},
    howpublished = {AI Alignment Forum},
    year = {2022},
    month = {12},
    note = {Interim research report},
    url = {https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition},
}

@article{cunningham2023sparse,
  title={Sparse autoencoders find highly interpretable features in language models},
  author={Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  journal={arXiv preprint arXiv:2309.08600},
  year={2023}
}

@article{whittington2022disentanglement,
  title={Disentanglement with biological constraints: A theory of functional cell types},
  author={Whittington, James CR and Dorrell, Will and Ganguli, Surya and Behrens, Timothy EJ},
  journal={arXiv preprint arXiv:2210.01768},
  year={2022}
}

@article{garfinkle2019uniqueness,
  title={On the uniqueness and stability of dictionaries for sparse representation of noisy signals},
  author={Garfinkle, Charles J and Hillar, Christopher J},
  journal={IEEE Transactions on Signal Processing},
  volume={67},
  number={23},
  pages={5884--5892},
  year={2019},
  publisher={IEEE}
}

@article{templeton2024scaling,
   title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
   author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
   year={2024},
   journal={Transformer Circuits Thread},
   url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
}

@article{marks2023interpreting,
  title={Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse Autoencoders},
  author={Marks, Luke and Abdullah, Amir and Mendez, Luna and Arike, Rauno and Torr, Philip and Barez, Fazl},
  journal={arXiv preprint arXiv:2310.08164},
  year={2023}
}

@article{kissane2024interpreting,
  title={Interpreting attention layer outputs with sparse autoencoders},
  author={Kissane, Connor and Krzyzanowski, Robert and Bloom, Joseph Isaac and Conmy, Arthur and Nanda, Neel},
  journal={arXiv preprint arXiv:2406.17759},
  year={2024}
}

@article{gorton2024missing,
  title={The Missing Curve Detectors of InceptionV1: Applying Sparse Autoencoders to InceptionV1 Early Vision},
  author={Gorton, Liv},
  journal={arXiv preprint arXiv:2406.03662},
  year={2024}
}

@article{rajamanoharan2024improving,
  title={Improving dictionary learning with gated sparse autoencoders},
  author={Rajamanoharan, Senthooran and Conmy, Arthur and Smith, Lewis and Lieberum, Tom and Varma, Vikrant and Kram{\'a}r, J{\'a}nos and Shah, Rohin and Nanda, Neel},
  journal={arXiv preprint arXiv:2404.16014},
  year={2024}
}

@article{dunefsky2024transcoders,
  title={Transcoders Find Interpretable LLM Feature Circuits},
  author={Dunefsky, Jacob and Chlenski, Philippe and Nanda, Neel},
  journal={arXiv preprint arXiv:2406.11944},
  year={2024}
}

@article{ferrando2024know,
  title={Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models},
  author={Ferrando, Javier and Obeso, Oscar and Rajamanoharan, Senthooran and Nanda, Neel},
  journal={arXiv preprint arXiv:2411.14257},
  year={2024}
}

@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{geva2023dissecting,
  title={Dissecting recall of factual associations in auto-regressive language models},
  author={Geva, Mor and Bastings, Jasmijn and Filippova, Katja and Globerson, Amir},
  journal={arXiv preprint arXiv:2304.14767},
  year={2023}
}

@article{turner2023activation,
  title={Activation addition: Steering language models without optimization},
  author={Turner, Alexander Matt and Thiergart, Lisa and Leech, Gavin and Udell, David and Vazquez, Juan J and Mini, Ulisse and MacDiarmid, Monte},
  journal={arXiv e-prints},
  pages={arXiv--2308},
  year={2023}
}

@article{meng2022locating,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

@misc{nanda2023fact,
    author = {Nanda, Neel and Rajamanoharan, Senthooran and Kramár, János and Shah, Rohin},
    title = {Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level},
    howpublished = {AI Alignment Forum},
    year = {2023},
    url = {https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall}
}

@article{geiger2020neural,
  title={Neural natural language inference models partially embed theories of lexical entailment and negation},
  author={Geiger, Atticus and Richardson, Kyle and Potts, Christopher},
  journal={arXiv preprint arXiv:2004.14623},
  year={2020}
}

@article{vig2020investigating,
  title={Investigating gender bias in language models using causal mediation analysis},
  author={Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12388--12401},
  year={2020}
}

@article{yuksekgonul2023attention,
  title={Attention satisfies: A constraint-satisfaction lens on factual errors of language models},
  author={Yuksekgonul, Mert and Chandrasekaran, Varun and Jones, Erik and Gunasekar, Suriya and Naik, Ranjita and Palangi, Hamid and Kamar, Ece and Nushi, Besmira},
  journal={arXiv preprint arXiv:2309.15098},
  year={2023}
}

@article{liu2024refining,
  title={Refining chatgpt-generated code: Characterizing and mitigating code quality issues},
  author={Liu, Yue and Le-Cong, Thanh and Widyasari, Ratnadira and Tantithamthavorn, Chakkrit and Li, Li and Le, Xuan-Bach D and Lo, David},
  journal={ACM Transactions on Software Engineering and Methodology},
  volume={33},
  number={5},
  pages={1--26},
  year={2024},
  publisher={ACM New York, NY}
}

@article{team2024gemma,
  title={Gemma 2: Improving open language models at a practical size},
  author={Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}

@article{lieberum2024gemma,
  title={Gemma scope: Open sparse autoencoders everywhere all at once on gemma 2},
  author={Lieberum, Tom and Rajamanoharan, Senthooran and Conmy, Arthur and Smith, Lewis and Sonnerat, Nicolas and Varma, Vikrant and Kram{\'a}r, J{\'a}nos and Dragan, Anca and Shah, Rohin and Nanda, Neel},
  journal={arXiv preprint arXiv:2408.05147},
  year={2024}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{jimenez2023swe,
  title={Swe-bench: Can language models resolve real-world github issues?},
  author={Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2310.06770},
  year={2023}
}

@inproceedings{krzyzanowski2024we,
  title={We inspected every head in gpt-2 small using saes so you don’t have to},
  author={Krzyzanowski, Robert and Kissane, Connor and Conmy, Arthur and Nanda, Neel},
  booktitle={Alignment Forum},
  year={2024}
}

@article{krasner2022cost,
  title={The cost of poor software quality in the US: A 2022 report},
  author={Krasner, Herb},
  journal={Consortium for Information \& Software Quality (CISQ)},
  year={2022}
}

@article{olah2017feature,
  title={Feature visualization},
  author={Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  journal={Distill},
  volume={2},
  number={11},
  pages={e7},
  year={2017}
}

@article{park2023linear,
  title={The linear representation hypothesis and the geometry of large language models},
  author={Park, Kiho and Choe, Yo Joong and Veitch, Victor},
  journal={arXiv preprint arXiv:2311.03658},
  year={2023}
}

@misc{anthropic2025claude,
  author = {Anthropic},
  title = {Claude 3.7 Sonnet and Claude Code},
  year = {2025},
  howpublished = {\url{https://www.anthropic.com/blog/claude-3-7-sonnet-and-claude-code}},
}

@misc{openai2025o3mini,
  author = {OpenAI},
  title = {OpenAI o3-mini: Pushing the frontier of cost-effective reasoning},
  year = {2025},
  howpublished = {\url{https://openai.com/index/openai-o3-mini/}},
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{tigges2023linear,
  title={Linear representations of sentiment in large language models},
  author={Tigges, Curt and Hollinsworth, Oskar John and Geiger, Atticus and Nanda, Neel},
  journal={arXiv preprint arXiv:2310.15154},
  year={2023}
}

@article{li2023inference,
  title={Inference-time intervention: Eliciting truthful answers from a language model},
  author={Li, Kenneth and Patel, Oam and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={41451--41530},
  year={2023}
}

@article{zou2023representation,
  title={Representation engineering: A top-down approach to ai transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}

@article{austin2021program,
  title={Program Synthesis with Large Language Models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{he2024dictionary,
  title={Dictionary learning improves patch-free circuit discovery in mechanistic interpretability: A case study on othello-gpt},
  author={He, Zhengfu and Ge, Xuyang and Tang, Qiong and Sun, Tianxiang and Cheng, Qinyuan and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2402.12201},
  year={2024}
}

@article{o2024sparse,
  title={Sparse autoencoders enable scalable and reliable circuit identification in language models},
  author={O'Neill, Charles and Bui, Thang},
  journal={arXiv preprint arXiv:2405.12522},
  year={2024}
}

@article{hendrycks2021measuring,
  title={Measuring coding challenge competence with apps},
  author={Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and others},
  journal={arXiv preprint arXiv:2105.09938},
  year={2021}
}

@article{hendrycksapps2021,
  title={Measuring Coding Challenge Competence With APPS},
  author={Dan Hendrycks and Steven Basart and Saurav Kadavath and Mantas Mazeika and Akul Arora and Ethan Guo and Collin Burns and Samir Puranik and Horace He and Dawn Song and Jacob Steinhardt},
  journal={NeurIPS},
  year={2021}
}

@article{mccabe1976complexity,
  title={A complexity measure},
  author={McCabe, Thomas J},
  journal={IEEE Transactions on software Engineering},
  number={4},
  pages={308--320},
  year={1976},
  publisher={IEEE}
}
